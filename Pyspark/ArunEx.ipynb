{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using sqoop, import orders table into hdfs to folders /user/cloudera/problem1/orders. \\\n",
    "File should be loaded as Avro File and use snappy compression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqoop import \\\n",
    "# > -Dmapreduce.job.user.classpath.first=true\n",
    "# > --connect jdbc:mysql://ms.itversity.com/retail_db \\\n",
    "# > --username retail_user \\\n",
    "# > --password itversity \\\n",
    "# > --table orders \\\n",
    "# > -m 2 \\\n",
    "# > --compression-codec 'snappy' \\\n",
    "# > --as-avrodatafile \\\n",
    "# > --target-dir /user/indirameduri/problem1/orders \\\n",
    "# > --delete-target-dir\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Arun3').master('yarn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('use indirameduri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.table('order_items')\n",
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from order_items').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders = spark.read.schema(\"order_id int ,order_date timestamp,order_cust_id int,order_status string\").csv(\"/public/retail_db/orders\")\n",
    "#('/public/retail_db/orders/',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find total orders and total amount per status per day. The result should be sorted by order date in descending, order status in ascending and total amount in descending and total orders in ascending.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions\n",
    "spark.sql('select distinct date_format(order_date,\"EE\") Day from orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions\n",
    "orders.createOrReplaceTempView('orders')\n",
    "order_items.createOrReplaceTempView('orderitems')\n",
    "data = spark.sql('''select  count(o.order_id) count,round(sum(oi.order_item_subtotal),2) total ,o.order_status,date_format(o.order_date,'yyyy-mm-dd') date from orders o join orderitems oi\n",
    "                    on o.order_id = oi.order_item_order_id group by date,o.order_status order by date desc, o.order_status, total desc, count''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet with gzip\n",
    "data.coalesce(2).write.option('compression','gzip').mode('overwrite').parquet('/user/indirameduri/arun')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet with snappy\n",
    "data.coalesce(2).write.option('compression','snappy').parquet('/user/indirameduri/arun/parquetsnappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv with no compression\n",
    "data.coalesce(3).write.option('compression','none').csv('/user/indirameduri/arun/csvnocompress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to hive table\n",
    "\n",
    "data.write.saveAsTable('arun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
