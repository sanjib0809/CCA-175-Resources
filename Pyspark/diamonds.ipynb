{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('yarn').appName('Diamondscsv').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the files are under local file system and we want to use Spark in YARN, we are using Pandas to create a Data Frame out of the local file on `gw03.itversity.com`.\n",
    "* If the file is too big, this approach is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diamonds_raw = pd.read_csv('/home/indirameduri/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once we have Pandas Data Frame, we can build Spark Data Frame using `spark.createDataFrame` where `spark` is instance of `pyspark.sql.SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Pandas Data Frame, we need not specify schema as Pandas also have schema associated with Data Frame.\n",
    "* Here is how we can create Spark Data Frame using Pandas Data Frame.\n",
    "* This approach is typically used to pass small files locally at the time of job submission and join with larger files to leverage distributed computing capabilities of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = spark.createDataFrame(diamonds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hostname -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('hdfs dfs -put diamonds.csv /user/indirameduri/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/indirameduri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('/user/indirameduri/diamonds.csv',header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding index column to the DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = data.withColumn('index',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 - Count how many diamonds are of each colour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.createOrReplaceTempView('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from diamonds').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with spark sql\n",
    "diamonds_cnt = spark.sql('select count(carat) count,color from diamonds group by color')\n",
    "diamonds_cnt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with DF\n",
    "diamonds_cnt = newData.groupBy('color').count()\n",
    "diamonds_cnt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 - Make a new row which consists of the first letter of 'cut' followed by the 'color', with a space in between the two**\n",
    "\n",
    "**So for example if Cut = Ideal and Color = E, there should be a new column called 'CutColor' with 'I E'.**\n",
    "\n",
    "**Also, replace the Cut and Color columns by the CutColor column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with withColoumn\n",
    "from pyspark.sql.functions import concat,substring,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = newData.withColumn('CutColor',concat(substring(newData.cut,0,1),lit(' '),newData.color))\n",
    "newData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with Spark sql\n",
    "spark.sql('select concat(substring(cut,0,1),\" \",color) CutColor from diamonds').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 - Create two tables and then re-join them\n",
    "Define D1 to be a table with just the index and carat. Define D2 to be just the index and the price. Then rejoin the two of them to get a table with index, carat, and price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = newData.select('index','carat')\n",
    "d1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = newData.select('index','price')\n",
    "d2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join d1 and d2\n",
    "d3 = d1.join(d2, d1.index==d2.index).select(d1.index,d1.carat,d2.price)\n",
    "d3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution with spark sql\n",
    "d1 = spark.sql('select index,carat from diamonds')\n",
    "d1.show()\n",
    "d1.createOrReplaceTempView('d1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2= spark.sql('select index,price from diamonds')\n",
    "d2.show()\n",
    "d2.createOrReplaceTempView('d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join in spark sql\n",
    "d3 = spark.sql('select d1.index,d1.carat,d2.price from d1 join d2 where d1.index=d2.index')\n",
    "d3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 - Multiply the dimensions to calculate volume, and then sort the entries by descending volume (with 2 decimal places).\n",
    "Tips: Look into the 'round' function, which can be used inside DF.select.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with DF\n",
    "from pyspark.sql.functions import round\n",
    "volume = newData.withColumn('volume',round((newData.x * newData.y * newData.z),2))\n",
    "volume.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution with spark sql\n",
    "volume = spark.sql('select *,round((x * y * z),2) volume from diamonds')\n",
    "volume.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 - Read in the Table as an RDD, Remove the Header, and Turn it into a DF\n",
    "You must manually define a schema using StructType, which is read in when creating the DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddData = newData.rdd.map(tuple)\n",
    "rddData.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"carat\",DoubleType(),True),\n",
    "    StructField(\"cut\",StringType(),True),\n",
    "    StructField(\"color\",StringType(),True),\n",
    "    StructField(\"clarity\",StringType(),True),\n",
    "    StructField(\"depth\",DoubleType(),True),\n",
    "    StructField(\"table\",DoubleType(),True),\n",
    "    StructField(\"price\",LongType(),True),\n",
    "    StructField(\"x\",DoubleType(),True),\n",
    "    StructField(\"y\",DoubleType(),True),\n",
    "    StructField(\"z\",DoubleType(),True),\n",
    "    StructField(\"index\",IntegerType(),True),\n",
    "    StructField(\"CutColor\",StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rddData,schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 - Reading in the Table as an DF, Output the Average Price Per Cut**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with spark sql\n",
    "\n",
    "avgPerCut = spark.sql('select round(avg(price),2) avg,cut from diamonds group By cut')\n",
    "avgPerCut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution with DF\n",
    "\n",
    "avgCut = newData.groupBy(newData.cut).agg({\"price\" : \"avg\"})\n",
    "avgCut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
