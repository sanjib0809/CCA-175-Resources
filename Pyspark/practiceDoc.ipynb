{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('practice').master('yarn').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('Alice',1)]\n",
    "spark.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=spark.createDataFrame(l)\n",
    "k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [{'name':'Alice','age':'1'}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(l)\n",
    "spark.createDataFrame(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd,['name','age'])\n",
    "df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('age','name')\n",
    "person=rdd.map(lambda r: Person(*r))\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",IntegerType(),True)])\n",
    "df3 = spark.createDataFrame(rdd,schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(df.toPandas()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "spark.createDataFrame(pandas.DataFrame([[1,2],[3,4]])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(rdd,\"a:string,b: int\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,7,2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(4).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.createOrReplaceTempView('table1')\n",
    "df2= spark.sql('select name as f1, age as f2 from table1')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.table(\"table1\")\n",
    "sorted(df2.collect()) == sorted(df3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strlen = spark.udf.register(\"stringlen\",lambda x: len(x))\n",
    "spark.sql('select stringlen(\"indira\")').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select 'foo' as text\").select(strlen(\"text\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "_ = spark.udf.register(\"stringLengthInt\", lambda x : len(x),IntegerType())\n",
    "spark.sql(\"select stringLengthInt('test')\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = spark.read.csv('/public/retail_db/departments').toDF('dept_id','dept_name')\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort(d.dept_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort(d.dept_name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort('dept_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort('dept_id',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc,desc\n",
    "d.orderBy('dept_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.orderBy(asc('dept_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort(desc('dept_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.orderBy(['dept_id',d.dept_name],ascending = [1,0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.agg({\"dept_id\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.agg({\"dept_id\":\"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "df = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=''), \\\n",
    "    Row(name='Alice', age='', height=80), \\\n",
    "    Row(name=\"\", age=10, height=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(99).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(88).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.write.format('json').mode('overwrite').save('/user/indirameduri/dept_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_json = spark.read.json('/user/indirameduri/dept_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('use indirameduri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create JSON in Hive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add jar /usr/hdp/2.6.5.0-292/hive2/lib/hive-hcatalog-core-2.1.0.2.6.5.0-292.jar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('use indirameduri')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spark.sql('use indirameduri') \\\n",
    "spark.sql('''create table orderitems_json(order_item_order_id int,order_item_product_id int,order_item_quantity int, \\\n",
    "            order_item_subtotal float,order_item_product_price float) \\\n",
    "            ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' \\\n",
    "            stored as textfile \\\n",
    "             location \"/user/indirameduri/json/order_items\"''').show()**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# points to remember for JSON\n",
    "# use row format serde 'jar file'\n",
    "# location 'hdfs path with folder only,no filename'\n",
    "# **column names in .json file has to exactly match create table command along with datatypes and names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Hive Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('use indirameduri')\n",
    "departments = spark.read.csv('/public/retail_db/departments',inferSchema=True).toDF('department_id','department_name')\n",
    "departments.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write to hive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.write.format('parquet').mode('overwrite').saveAsTable('departments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders = spark.read.table('indirameduri.orders')\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.partitionBy('order_status').bucketBy(16,'order_id').saveAsTable('indirameduri.orders2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from orders1 limit 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1,10))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenRdd = rdd.filter(lambda x:x%2==0)\n",
    "evenRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddRdd = rdd.filter(lambda y:y%2!=0)\n",
    "oddRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = spark.read.json('/user/indirameduri/new/file1.json')\n",
    "file1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.write.json('/user/indirameduri/new/file2.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -getmerge -nl /user/indirameduri/employee/ file3 for merging file in source directory to destination local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**append to existing table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.write.mode('append').saveAsTable('departments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from departments').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = sc.parallelize(List(\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"), 3)\n",
    "val b = a.keyBy(_.length)\n",
    "val c = sc.parallelize(Ust(\n",
    "val d = c.keyBy(_.length) operation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(list([\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"]),3)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alen = a.map(lambda x: (len(x),x))\n",
    "alen.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sc.parallelize(list([\"dog\",\"cat\",\"gnu\",\"salmon\",\"rabbit\",\"turkey\",\"wolf\",\"bear\",\"bee\"]), 3)\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clen=c.map(lambda x: (len(x),x))\n",
    "clen.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(range(1,100),3)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = spark.read.csv('/user/indirameduri/new/file4')\n",
    "file1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = spark.read.csv('/user/indirameduri/new/file5')\n",
    "file2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Aggregations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('/public/retail_db/orders/',inferSchema=True).toDF('order_id','order_date','order_cust_id','order_status')\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from orders limit 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems = spark.read.csv('/public/retail_db/order_items',inferSchema=True).toDF('oi_id','oi_order_id','oi_product_id','oi_quantity','oi_subtotal','oi_price')\n",
    "orderItems.show()\n",
    "orderItems.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "spec=Window.partitionBy(orderItems.oi_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems.withColumn('order_sum',sum(orderItems.oi_subtotal).over(spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems. \\\n",
    "withColumn('order_sum',sum(orderItems.oi_subtotal).over(spec)). \\\n",
    "withColumn('order_max',max(orderItems.oi_subtotal).over(spec)). \\\n",
    "withColumn('order_min',min(orderItems.oi_subtotal).over(spec)). \\\n",
    "withColumn('order_avg',avg(orderItems.oi_subtotal).over(spec)). \\\n",
    "withColumn('order_count',count(orderItems.oi_subtotal).over(spec)). \\\n",
    "withColumn('order_count1',count(lit(\"1\")).over(spec)). \\\n",
    "drop('oi_product_id','oi_quantity','oi_price'). \\\n",
    "orderBy('oi_order_id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems.createOrReplaceTempView('orderitems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select oi_order_id,round(sum(oi_subtotal),2) sum from orderitems group by oi_order_id ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select round(sum(oi_subtotal),2) over(partition by oi_order_id) from orderitems').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [['Thin','Cell Phone',6000],\n",
    "      ['Normal','Tablet',1500],\n",
    "      ['Mini','Tablet',5500],\n",
    "      ['Ultra Thin','Cell Phone',5000],\n",
    "      ['Very Thin','Cell Phone' ,6000],\n",
    "      ['Big','Tablet',2500],\n",
    "      ['Bendable','Cell Phone',3000],\n",
    "      ['Foldable','Cell Phone',3000],\n",
    "      ['Pro','Tablet',4500],\n",
    "      ['Pro2','Tablet',6500]]\n",
    "df = spark.createDataFrame(df).toDF('Product','Category','Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the best-selling and the second best-selling products in every category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select Category,max(Revenue) rev from pr group by Category order by rev desc limit 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select Product,Category,Revenue from (select Product,Category,Revenue,\n",
    "            dense_rank() over(partition by Category order by Revenue desc) rank from pr) tmp where rank<=2''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between the revenue of each product and \\\n",
    "the revenue of the best-selling product in the same category of that product?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "windowSpec = Window.partitionBy(df.Category).orderBy(df.Revenue.desc()). \\\n",
    "                rangeBetween(-sys.maxsize,sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_diff = (f.max(df.Revenue).over(windowSpec)-df.Revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Product','Category','Revenue',rev_diff.alias('Rev Diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
