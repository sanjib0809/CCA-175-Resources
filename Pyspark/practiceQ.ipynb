{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/indirameduri/import-all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('AnoosaShetty').config('spark.ui.port', '0').master('yarn').getOrCreate()\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Most selling product (But Quantity not by Cost) for every month in the database (Between July 2013 to July 2014)**\n",
    "**2) Who are the top 10 revenue generating customers?**\n",
    "**3) What are the top 10 revenue generating products?**\n",
    "**4) Top 5 revenue generating deparments;**\n",
    "**5) Top 5 revenue generating cities (from address of Customers)**\n",
    "\n",
    "**Please provide answers in Scala or Python or Spark SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.format('csv').load('/user/indirameduri/import-all/orders',inferSchema='True').toDF('order_id','order_date','order_customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.read.csv('/user/indirameduri/import-all/categories',inferSchema='true').toDF('category_id','category_dept_id','category_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = spark.read.csv('/user/indirameduri/import-all/departments',inferSchema='true').toDF('department_id','department_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.csv('/user/indirameduri/import-all/products',inferSchema='true').toDF('product_id','prod_category_id', \\\n",
    "                                                                                           'product_name','product_desc','product_price','product_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.csv('/user/indirameduri/import-all/order_items').toDF('order_item_id','oi_order_id','oi_product_id','oi_quantity','oi_subtotal','oi_product_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers=spark.read.csv('/user/indirameduri/import-all/customers',inferSchema='true').toDF('customer_id','customer_fname','customer_lname','customer_email','customer_password','customer_street','customer_city','customer_state','customer_zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1) Most selling product (But Quantity not by Cost) for every month in the database (Between July 2013 to July 2014)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join between orders, order_items, products,  with group by on products and count of qty\n",
    "\n",
    "tJoins = orders.join(order_items,orders.order_id==order_items.oi_order_id).join(products,order_items.oi_product_id == products.product_id).filter(orders.order_date.between(\"2013-07%\",\"2014-07%\"))\n",
    "                                                                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tJoins.show()\n",
    "tJoins.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tJoins.groupBy('product_name').count().sort('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using spark sql\n",
    "from pyspark.sql.functions import date_format\n",
    "tJoins.createOrReplaceTempView('data')\n",
    "mostSoldProduct = spark.sql('select count(oi_quantity) count,product_name from data group by product_name order by count(oi_quantity) desc')\n",
    "mostSoldProduct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('desc data').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Who are the top 10 revenue generating customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(subtotal) , joins between customer.customer_id = orders.orders_customer_id and orders.order_id = order_items.oi_order_id\n",
    "#group by customer_id\n",
    "from pyspark.sql.functions import round\n",
    "data = order_items.join(orders, orders.order_id==order_items.oi_order_id).join(customers, customers.customer_id==orders.order_customer_id)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f \n",
    "topCustomers=data.groupBy(data.customer_id).agg(f.round(f.sum(\"oi_subtotal\"),2).alias('Total Amount')).orderBy('Total Amount',ascending=False).limit(10)\n",
    "topCustomers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topNCustomers = data.groupBy(data.customer_id).agg({\"oi_subtotal\":\"sum\"}).limit(10)\n",
    "topNCustomers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With spark sql\n",
    "from pyspark.sql.functions import round,sum\n",
    "topCustomers_sql = spark.sql('''select data.order_customer_id,round(sum(data.oi_subtotal),2) Total_Amount from  data\n",
    "                                group by data.order_customer_id\n",
    "                                order by round(sum(data.oi_subtotal),2) desc\n",
    "                                limit 10''')\n",
    "\n",
    "topCustomers_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) What are the top 10 revenue generating products?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(subtotal) with joins on products, orders,order_items\n",
    "\n",
    "data = orders.join(order_items,orders.order_id==order_items.oi_order_id).join(products,products.product_id==order_items.oi_product_id)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "topNProducts = data.groupBy('product_name').agg(f.round(f.sum('oi_subtotal')).alias('Total Amount')).orderBy('Total Amount',ascending=False)\n",
    "topNProducts.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark sql \n",
    "import pyspark.sql.functions\n",
    "data.createOrReplaceTempView('topProducts')\n",
    "topNProducts = spark.sql('select data.product_name,round(sum(data.oi_subtotal),2) Total_Amount from data group by data.product_name order by round(sum(data.oi_subtotal),2) desc limit 10')\n",
    "topNProducts.show()                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Top 5 revenue generating deparments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joins between 4 tables order_items,departments,products,categories\n",
    "\n",
    "data = departments.join(categories,departments.department_id == categories.category_dept_id).join(products,categories.category_id==products.prod_category_id).join(order_items,products.product_id==order_items.oi_product_id)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "topDepts = data.groupBy('department_name').agg(f.round(f.sum('oi_subtotal'),2).alias('Total Amount')).orderBy('Total Amount',ascending=False)\n",
    "topDepts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Top 5 revenue generating cities (from address of Customers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joins between customers,orders,order_items\n",
    "\n",
    "data = customers.join(orders,customers.customer_id==orders.order_customer_id).join(order_items,order_items.oi_order_id==orders.order_id)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "top5Cities = data.groupBy('customer_city').agg(f.round(f.sum('oi_subtotal'),2).alias('Total Amount')).orderBy('Total Amount',ascending=False).limit(5)\n",
    "top5Cities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark sql \n",
    "import pyspark.sql.functions\n",
    "data.createOrReplaceTempView('cities')\n",
    "top5Cities = spark.sql('select customer_city, round(sum(oi_subtotal),2) Total_Amount from cities group by customer_city order by Total_Amount desc limit 5')\n",
    "top5Cities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
