{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is available in HDFS /public/randomtextwriter\n",
    "Get word count for the input data using space as delimiter (for each word, we need to get how many types it is repeated in the entire input data set)\n",
    "Number of executors should be 10\n",
    "Executor memory should be 3 GB\n",
    "Executor cores should be 20 in total (2 per executor)\n",
    "Number of output files should be 8\n",
    "Avro dependency details: groupId -> com.databricks, artifactId -> spark-avro_2.10, version -> 2.0.1\n",
    "Target Directory: /user/<YOUR_USER_ID>/solutions/solution05/wordcount\n",
    "Target File Format: Avro\n",
    "Target fields: word, count\n",
    "Compression: N/A or default**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Count using SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('wordcount').master('yarn').getOrCreate()\n",
    "data = spark.read.csv('/public/randomtextwriter/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode,split,col\n",
    "words = data.withColumn('words', explode(split('_c0', ' '))).groupBy('words').count().orderBy('count',ascending=False)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose language of your choice Python or Scala\n",
    "Data is available in HDFS file system under /public/crime/csv\n",
    "You can check properties of files using hadoop fs -ls -h /public/crime/csv\n",
    "Structure of data (ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)\n",
    "File format - text file\n",
    "Delimiter - “,”\n",
    "Get monthly count of primary crime type, sorted by month in ascending and number of crimes per type in descending order\n",
    "Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution01/crimes_by_type_by_month\n",
    "Output File Format: TEXT\n",
    "Output Columns: Month in YYYYMM format, crime count, crime type\n",
    "Output Delimiter: \\t (tab delimited)\n",
    "Output Compression: gzip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /public/crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('crimedata').config('spark.ui.port','0').master('yarn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#crimedata = spark.read.csv('file:///data/crime/csv',header='True',inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/crime/csv/rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
